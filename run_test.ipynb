{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cai Selvas Sala\\GIA_UPC\\Personal\\DatathonFME\\Datathon 2024\\handle\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from src.custom_inference_dataset import CustomInferenceDataset\n",
    "from src.handler import Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/x_num_categories_list.pkl', 'rb') as f:\n",
    "\tx_num_categories_list = pickle.load(f)\n",
    "\n",
    "with open('./data/y_num_categories_list.pkl', 'rb') as f:\n",
    "\ty_num_categories_list = pickle.load(f)\n",
    "\n",
    "with open('./data/label_encoders.pkl', 'rb') as f:\n",
    "\tlabel_encoders = pickle.load(f)\n",
    "\n",
    "with open('./data/onehot_encoders.pkl', 'rb') as f:\n",
    "\tone_hot_encoders = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cai Selvas Sala\\AppData\\Local\\Temp\\ipykernel_17216\\414790742.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./models/best_model.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Handler(\n",
       "  (image_encoder): ImageEncoder(\n",
       "    (model): CLIPModel(\n",
       "      (text_model): CLIPTextTransformer(\n",
       "        (embeddings): CLIPTextEmbeddings(\n",
       "          (token_embedding): Embedding(49408, 512)\n",
       "          (position_embedding): Embedding(77, 512)\n",
       "        )\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPSdpaAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "          (position_embedding): Embedding(50, 768)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPSdpaAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "      (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (tabular_encoder): TabularEncoder(\n",
       "    (embeddings): ModuleList(\n",
       "      (0): Embedding(3, 512)\n",
       "      (1): Embedding(5, 512)\n",
       "      (2): Embedding(3, 512)\n",
       "      (3): Embedding(7, 512)\n",
       "      (4): Embedding(5, 512)\n",
       "      (5): Embedding(26, 512)\n",
       "      (6): Embedding(47, 512)\n",
       "      (7): Embedding(122, 512)\n",
       "    )\n",
       "    (transformer_layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer_norms): ModuleList(\n",
       "      (0-1): 2 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multimodal_interaction): MultimodalInteraction(\n",
       "    (cross_attention_layers): ModuleList(\n",
       "      (0-1): 2 x MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (feed_forward_layers): ModuleList(\n",
       "      (0-1): 2 x Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norms_attn): ModuleList(\n",
       "      (0-1): 2 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layer_norms_ff): ModuleList(\n",
       "      (0-1): 2 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (output_layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=512, out_features=7, bias=True)\n",
       "      (2): Linear(in_features=512, out_features=12, bias=True)\n",
       "      (3): Linear(in_features=512, out_features=9, bias=True)\n",
       "      (4): Linear(in_features=512, out_features=13, bias=True)\n",
       "      (5-6): 2 x Linear(in_features=512, out_features=34, bias=True)\n",
       "      (7): Linear(in_features=512, out_features=7, bias=True)\n",
       "      (8-10): 3 x Linear(in_features=512, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Handler(x_num_categories_list=x_num_categories_list, y_num_categories_list=y_num_categories_list)\n",
    "\n",
    "model.load_state_dict(torch.load('./models/best_model.pth', map_location=device))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = './data/archive/images/images'\n",
    "x_test_file_path = './data/x_test_final.csv'\n",
    "y_data_file_path = './data/y_full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = pd.read_csv(y_data_file_path).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomInferenceDataset(label_encoders=label_encoders, onehot_encoders=one_hot_encoders, x_test_path=x_test_file_path, image_folder_path=image_folder_path)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 224, 160])\n",
      "torch.Size([1, 8])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Inicializa una lista para almacenar los resultados\u001b[39;00m\n\u001b[0;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtabular_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mtabular_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtabular_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\Personal\\DatathonFME\\Datathon 2024\\handle\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\Personal\\DatathonFME\\Datathon 2024\\handle\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\Personal\\DatathonFME\\Datathon 2024\\handle\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\Personal\\DatathonFME\\Datathon 2024\\handle\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\Personal\\DatathonFME\\Datathon 2024\\handle\\src\\custom_inference_dataset.py:53\u001b[0m, in \u001b[0;36mCustomInferenceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m \timg_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_folder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_names[idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 53\u001b[0m \timage \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \timage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m     56\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\Personal\\DatathonFME\\Datathon 2024\\handle\\.venv\\Lib\\site-packages\\PIL\\Image.py:3480\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3477\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m   3478\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 3480\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m   3482\u001b[0m preinit()\n\u001b[0;32m   3484\u001b[0m warning_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Inicializa una lista para almacenar los resultados\n",
    "results = []\n",
    "\n",
    "for image, tabular_data, attribute_name, test_id in test_loader:\n",
    "\timage = image.to(device)\n",
    "\ttabular_data = tabular_data.to(device)\n",
    "\n",
    "\ttest_id = test_id[0]\n",
    "\tattribute_name = attribute_name[0]\n",
    "\n",
    "\tprint(image.shape)\n",
    "\tprint(tabular_data.shape)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\ttry:\n",
    "\t\t\tpred = model(image, tabular_data)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f'Error predicting for test_id {test_id}. Error message: {e}')\n",
    "\t\t\tresults.append({'test_id': test_id, 'des_value': 'INVALID'})\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t# Convertir predicciones a índices de clases\n",
    "\tpred_classes = [torch.argmax(attr_pred, dim=1).cpu().numpy() for attr_pred in pred]\n",
    "\n",
    "\t# Obtener el índice del atributo\n",
    "\tattr_index = y_labels.index(attribute_name)\n",
    "\n",
    "\t# Crear un vector one-hot para el índice predicho\n",
    "\tnum_categories = len(test_dataset.onehot_encoders[attribute_name].categories_[0])\n",
    "\tonehot_vector = [0] * num_categories\n",
    "\tonehot_vector[pred_classes[attr_index][0]] = 1\n",
    "\t\n",
    "\t# Decodifica el valor de la clase predicha\n",
    "\tpred_value = test_dataset.value_from_onehot_encoder(attribute_name, onehot_vector)\n",
    "\n",
    "\t# Agrega el resultado a la lista\n",
    "\tresults.append({'test_id': test_id, 'des_value': pred_value})\n",
    "\n",
    "# Convierte la lista de resultados en un DataFrame\n",
    "resulting_df = pd.DataFrame(results)\n",
    "\n",
    "# Guarda el DataFrame en un archivo CSV\n",
    "resulting_df.to_csv('./data/test_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
